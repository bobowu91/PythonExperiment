{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import login  # a python file that contains my login information\n",
    "import time\n",
    "import json\n",
    "\n",
    "path = r'/Users/yibowang/Documents/Chromedriver/chromedriver'\n",
    "login_page = 'https://www.lib.uwo.ca/cgi-bin/ezpauthn.cgi?url=http://global.factiva.com/en/sess/login.asp?xsid=S003WvtZWzk5DEs5DEmM9EpODaoOT7yMHmnRsIuMcNG1pRRQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQQAA'\n",
    "username = login.username\n",
    "password = login.password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a web browser and go to the url in the browser\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('headless')\n",
    "driver = webdriver.Chrome(path, options=options)\n",
    "driver.get(login_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log in to the website to create one session\n",
    "user = driver.find_element_by_name('user')\n",
    "user.send_keys(username)\n",
    "pin = driver.find_element_by_name('pass')\n",
    "pin.send_keys(password)\n",
    "login_button = driver.find_element_by_xpath(\n",
    "    '/html/body/center[2]/div/form/p[4]/input')\n",
    "login_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langding page: lp = \"https://global-factiva-com.proxy1.lib.uwo.ca/sb/default.aspx?lnep=hp\"\n",
    "# time.sleep(50) this log in process roughly takes about 45 seconds\n",
    "home = driver.find_element_by_link_text(\"Home\")\n",
    "home.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time.sleep(10) this clicking process roughly takes about 10 seconds\n",
    "viewall = driver.find_element_by_link_text(\"View All\")\n",
    "viewall.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time.sleep(15) this clicking process roughly takes about 18 seconds\n",
    "html_doc = []\n",
    "all_title = []\n",
    "for index in range(1,60):\n",
    "    xpath = '//*[@id=\"headlines\"]/table/tbody/tr[' + str(index) + ']/td[3]/a' # print(xpath)\n",
    "    headline = driver.find_element_by_xpath(xpath)\n",
    "    headline.click()\n",
    "    all_title.append(headline.text)\n",
    "    # time.sleep(3) # previously was set to 2 seconds but it might be too short since loading takes more than that\n",
    "    \n",
    "    # use bs4 to parse text information \n",
    "    article_html = driver.page_source\n",
    "    article = BeautifulSoup(article_html, \"html.parser\")\n",
    "    html_doc.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj = []\n",
    "for c in html_doc:\n",
    "    if c.find(\"div\", {\"class\": \"author\"}) != None:   \n",
    "        news = {}\n",
    "        news[\"paragraph\"] = c.find_all(\"p\", {\"class\": \"articleParagraph enarticleParagraph\"}) # a list of paragraphs\n",
    "        news[\"title\"] = c.find(\"span\", {\"class\": \"enHeadline\"})\n",
    "        # print(news[\"title\"]) for illustrating purpose\n",
    "        author = c.find(\"div\", {\"class\": \"author\"})\n",
    "        news[\"author\"] = author\n",
    "        news[\"others\"] = author.find_next_siblings(\"div\") # information about the article; others[2] is date\n",
    "        wsj.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal = []\n",
    "for c in html_doc:\n",
    "    if c.find(\"span\", {\"class\": \"enHeadline\"}) != None:   \n",
    "        news = {}\n",
    "        title = c.find(\"span\", {\"class\": \"enHeadline\"})\n",
    "        news[\"title\"] = title.text\n",
    "        \n",
    "        hd = c.find(\"div\", {\"id\": \"hd\"})\n",
    "        others = hd.find_next_siblings(\"div\") # information about the article; others[2] is date\n",
    "        news[\"author\"] = others[0].text\n",
    "        news[\"wordcount\"] = others[2].text\n",
    "        news[\"date\"] = others[3].text\n",
    "        \n",
    "        paragraphs = c.find_all(\"p\", {\"class\": \"articleParagraph enarticleParagraph\"}) # a list of paragraphs\n",
    "        news[\"paragraph\"] = [p.text for p in paragraphs]\n",
    "        \n",
    "\n",
    "        journal.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for i in wsj:\n",
    "    print(i[\"title\"].text)\n",
    "    print(i[\"others\"][0].text)\n",
    "    print(i[\"others\"][2].text)\n",
    "    print(\"-----------------------------------\")\n",
    "    for paragraph in i[\"paragraph\"]:\n",
    "        print(paragraph.text)\n",
    "        print(\"\\n\")\n",
    "    print(\"***************************************\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for read in journal[31:37]:\n",
    "    print(read['title'])\n",
    "    print(read['author'])\n",
    "    print(read['date'])\n",
    "    print(\"-----------------------------------\")\n",
    "    for i in read['paragraph']:\n",
    "        print(i)\n",
    "        print('\\n')\n",
    "    print(\"***************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is not good;\n",
    "\"\"\"with open(\"pywsj_content.json\", \"w\") as outfile:\n",
    "    for news_article in journal:\n",
    "        json.dump(news_article, outfile)\n",
    "        outfile.write('\\n')\"\"\"\n",
    "\n",
    "## simple load the list of dictionaries into json\n",
    "with open(\"pywsj_may14.json\", \"w\") as output:\n",
    "    json.dump(journal, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
